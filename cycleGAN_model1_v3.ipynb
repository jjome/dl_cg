{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjome/dl_cg/blob/master/cycleGAN_model1_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKzJ0QD23lC_",
        "outputId": "406dd876-893e-4e2c-dfa3-492a741bd11b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision pillow numpy matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJSlUqHVjcQw"
      },
      "source": [
        "### Colab 마운트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CuJp_EpC3mck"
      },
      "outputs": [],
      "source": [
        "# # This mounts your Google Drive to the Colab VM.\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# FOLDERNAME = 'test'\n",
        "# assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# import sys\n",
        "# sys.path.append('/content/drive/MyDrive/{}'.format(FOLDERNAME))\n",
        "\n",
        "# # Change directory to current folder\n",
        "# %cd /content/drive/MyDrive/$FOLDERNAME"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jjome/dl_cg.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Un3aUhAWF_e",
        "outputId": "8f01782e-0897-445d-fb77-a72315faa8f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'dl_cg' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3nwJu5_je57"
      },
      "source": [
        "필요한 라이브러리 import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wxUTF-cX3oCq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import itertools\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOvwjNXfjiZ3"
      },
      "source": [
        "### 사진 파일 불러오기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0fQb7eojlFS"
      },
      "source": [
        "'dataset' 파일 안에 'older' 파일과 'younger' 파일이 있는 형태여야 함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vqnm0IHJ3rpB"
      },
      "outputs": [],
      "source": [
        "# Define the base directories\n",
        "target_dir = '/content/dl_cg/dataset4'\n",
        "older_dir = os.path.join(target_dir, 'older')\n",
        "younger_dir = os.path.join(target_dir, 'younger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j5woZwejsRw"
      },
      "source": [
        "### transformations, dataloaders, batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0-4QGw54KyH",
        "outputId": "a8530144-5d0e-41a7-c7ff-27401dd1ca5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of older image batch: torch.Size([32, 3, 256, 256])\n",
            "Shape of younger image batch: torch.Size([32, 3, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Run the PyTorch code\n",
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    #transforms.RandomRotation(degrees=10),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Define dataset and dataloaders\n",
        "older_dataset = datasets.ImageFolder(root=older_dir, transform=transform)\n",
        "younger_dataset = datasets.ImageFolder(root=younger_dir, transform=transform)\n",
        "\n",
        "# Ensure drop_last=True to handle incomplete batches\n",
        "batch_size = 32\n",
        "older_dataloader = DataLoader(older_dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
        "younger_dataloader = DataLoader(younger_dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "# Example: Print first few images to verify\n",
        "older_img, older_label = next(iter(older_dataloader))\n",
        "younger_img, younger_label = next(iter(younger_dataloader))\n",
        "\n",
        "print(\"Shape of older image batch:\", older_img.shape)\n",
        "print(\"Shape of younger image batch:\", younger_img.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cmT7ySMjyLW"
      },
      "source": [
        "### generator, discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WlUwu-yG4VBL"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define the Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "# Define the Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMKDEBN1j1Wn"
      },
      "source": [
        "### cycleGAN 정의하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mi-07bt4WiQ",
        "outputId": "b59a1e70-a67f-40ee-bf2a-2ea5bfc788f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "class CycleGAN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CycleGAN, self).__init__()\n",
        "        self.G_XtoY = Generator()\n",
        "        self.G_YtoX = Generator()\n",
        "        self.D_X = Discriminator()\n",
        "        self.D_Y = Discriminator()\n",
        "\n",
        "    def forward(self):\n",
        "        pass\n",
        "\n",
        "# Instantiate the model and move it to the appropriate device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "cycle_gan = CycleGAN().to(device)\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMepj-kij4y2"
      },
      "source": [
        "### loss function, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BgGGp39v4Yc7"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import torch.optim as optim\n",
        "\n",
        "# Loss functions\n",
        "criterion_GAN = nn.MSELoss()\n",
        "criterion_cycle = nn.L1Loss()\n",
        "criterion_identity = nn.L1Loss()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = optim.Adam(itertools.chain(cycle_gan.G_XtoY.parameters(), cycle_gan.G_YtoX.parameters()), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D_X = optim.Adam(cycle_gan.D_X.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "optimizer_D_Y = optim.Adam(cycle_gan.D_Y.parameters(), lr=0.0001, betas=(0.5, 0.999))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET0FiQM5j7qO"
      },
      "source": [
        "### model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXprUH6f4aaV",
        "outputId": "ab3c2533-5be5-4327-c6d3-91570ba90b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0/5] [Batch 0/66] [D loss: 0.5256150960922241] [G loss: 12.734230995178223]\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "# num_epochs = 100\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (data_X, data_Y) in enumerate(zip(older_dataloader, younger_dataloader)):\n",
        "        # Get real images\n",
        "        real_X = data_X[0].to(device)\n",
        "        real_Y = data_Y[0].to(device)\n",
        "\n",
        "        # Adversarial ground truths with the same shape as discriminator outputs\n",
        "        valid = torch.ones(real_X.size(0), 1, 15, 15, dtype=torch.float, requires_grad=False).to(device)\n",
        "        fake = torch.zeros(real_X.size(0), 1, 15, 15, dtype=torch.float, requires_grad=False).to(device)\n",
        "\n",
        "        # ----------------------\n",
        "        #  Train Generators\n",
        "        # ----------------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Identity loss\n",
        "        loss_id_X = criterion_identity(cycle_gan.G_YtoX(real_X), real_X)\n",
        "        loss_id_Y = criterion_identity(cycle_gan.G_XtoY(real_Y), real_Y)\n",
        "\n",
        "        loss_identity = (loss_id_X + loss_id_Y) / 2\n",
        "\n",
        "        # GAN loss\n",
        "        fake_Y = cycle_gan.G_XtoY(real_X)\n",
        "        loss_GAN_XtoY = criterion_GAN(cycle_gan.D_Y(fake_Y), valid)\n",
        "\n",
        "        fake_X = cycle_gan.G_YtoX(real_Y)\n",
        "        loss_GAN_YtoX = criterion_GAN(cycle_gan.D_X(fake_X), valid)\n",
        "\n",
        "        loss_GAN = (loss_GAN_XtoY + loss_GAN_YtoX) / 2\n",
        "\n",
        "        # Cycle loss\n",
        "        recov_X = cycle_gan.G_YtoX(fake_Y)\n",
        "        loss_cycle_X = criterion_cycle(recov_X, real_X)\n",
        "\n",
        "        recov_Y = cycle_gan.G_XtoY(fake_X)\n",
        "        loss_cycle_Y = criterion_cycle(recov_Y, real_Y)\n",
        "\n",
        "        loss_cycle = (loss_cycle_X + loss_cycle_Y) / 2\n",
        "\n",
        "        # Total loss\n",
        "        loss_G = loss_GAN + 10.0 * loss_cycle + 5.0 * loss_identity\n",
        "\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # Adjust output range of generated images\n",
        "        fake_Y = (fake_Y + 1) / 2  # Map from [-1, 1] to [0, 1]\n",
        "        fake_X = (fake_X + 1) / 2  # Map from [-1, 1] to [0, 1]\n",
        "\n",
        "        # ----------------------\n",
        "        #  Train Discriminators\n",
        "        # ----------------------\n",
        "\n",
        "        # Discriminator X\n",
        "        optimizer_D_X.zero_grad()\n",
        "\n",
        "        loss_real_X = criterion_GAN(cycle_gan.D_X(real_X), valid)\n",
        "        loss_fake_X = criterion_GAN(cycle_gan.D_X(fake_X.detach()), fake)\n",
        "\n",
        "        loss_D_X = (loss_real_X + loss_fake_X) / 2\n",
        "\n",
        "        loss_D_X.backward()\n",
        "        optimizer_D_X.step()\n",
        "\n",
        "        # Discriminator Y\n",
        "        optimizer_D_Y.zero_grad()\n",
        "\n",
        "        loss_real_Y = criterion_GAN(cycle_gan.D_Y(real_Y), valid)\n",
        "        loss_fake_Y = criterion_GAN(cycle_gan.D_Y(fake_Y.detach()), fake)\n",
        "\n",
        "        loss_D_Y = (loss_real_Y + loss_fake_Y) / 2\n",
        "\n",
        "        loss_D_Y.backward()\n",
        "        optimizer_D_Y.step()\n",
        "\n",
        "        print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(older_dataloader)}] \"\n",
        "              f\"[D loss: {loss_D_X.item() + loss_D_Y.item()}] [G loss: {loss_G.item()}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJAT1JUuqam_"
      },
      "outputs": [],
      "source": [
        "cycle_gan.eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flaH_pXvj_Qy"
      },
      "source": [
        "## training한 모델 저장하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBOXzD3M4oge"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "torch.save(cycle_gan.state_dict(), '/content/drive/MyDrive/deep_learning/cycle_gan_model2.pth')\n",
        "torch.save(cycle_gan.state_dict(), 'cycle_gan_state_dict.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14JZOEIFkB7G"
      },
      "source": [
        "### trained된 모델로 이미지 넣어 결과물 보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xGUj9ZrkJOE"
      },
      "source": [
        "우선 저장된 모델 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQC0PY14hvyy",
        "outputId": "10c976a4-c282-486d-955d-ed7e34ddd94e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CycleGAN(\n",
              "  (G_XtoY): Generator(\n",
              "    (main): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (4): ReLU(inplace=True)\n",
              "      (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (7): ReLU(inplace=True)\n",
              "      (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (10): ReLU(inplace=True)\n",
              "      (11): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (13): ReLU(inplace=True)\n",
              "      (14): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (16): ReLU(inplace=True)\n",
              "      (17): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (19): ReLU(inplace=True)\n",
              "      (20): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (21): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (G_YtoX): Generator(\n",
              "    (main): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (4): ReLU(inplace=True)\n",
              "      (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (7): ReLU(inplace=True)\n",
              "      (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (10): ReLU(inplace=True)\n",
              "      (11): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (13): ReLU(inplace=True)\n",
              "      (14): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (16): ReLU(inplace=True)\n",
              "      (17): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (19): ReLU(inplace=True)\n",
              "      (20): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (21): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (D_X): Discriminator(\n",
              "    (main): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
              "      (12): Sigmoid()\n",
              "    )\n",
              "  )\n",
              "  (D_Y): Discriminator(\n",
              "    (main): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
              "      (12): Sigmoid()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Assuming you have defined your CycleGAN model class (CycleGAN) and its architecture\n",
        "\n",
        "# Instantiate the CycleGAN model\n",
        "cycle_gan = CycleGAN()\n",
        "\n",
        "# Load the trained model weights\n",
        "model_path = '/content/drive/MyDrive/test/cycle_gan_model1.pth'\n",
        "\n",
        "cycle_gan.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "cycle_gan.eval()\n",
        "\n",
        "# Move the model to CUDA if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cycle_gan.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC2Ww7m9kMSy"
      },
      "source": [
        "input 이미지 사진 transform하고 tensor로 변환하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7_eAuFth62r"
      },
      "outputs": [],
      "source": [
        "# Define transformations for the input image\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # Resize to the model's input size\n",
        "    transforms.ToTensor(),  # Convert PIL image to tensor\n",
        "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
        "    #transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Load and preprocess the test image\n",
        "input_image_path = '/content/drive/MyDrive/deep_learning/older_face1.jpg'\n",
        "input_image = Image.open(input_image_path).convert('RGB')\n",
        "input_tensor = transform(input_image).unsqueeze(0).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPSDyEi8WrO7",
        "outputId": "6471ea32-abfe-4fe7-e19b-6bb47b82bc78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[-0.1301, -0.1371, -0.1335,  ..., -0.1367, -0.1336, -0.1269],\n",
              "          [-0.1341, -0.0983, -0.1355,  ..., -0.0975, -0.1352, -0.1116],\n",
              "          [-0.1406, -0.1379, -0.1421,  ..., -0.1429, -0.1411, -0.1321],\n",
              "          ...,\n",
              "          [-0.1333, -0.0975, -0.1374,  ..., -0.0978, -0.1363, -0.1096],\n",
              "          [-0.1404, -0.1404, -0.1395,  ..., -0.1445, -0.1398, -0.1290],\n",
              "          [-0.1384, -0.1082, -0.1397,  ..., -0.1106, -0.1406, -0.1214]],\n",
              "\n",
              "         [[-0.1050, -0.1187, -0.1042,  ..., -0.1212, -0.1020, -0.1065],\n",
              "          [-0.1124, -0.0901, -0.0928,  ..., -0.0915, -0.0897, -0.0891],\n",
              "          [-0.1186, -0.1476, -0.1287,  ..., -0.1433, -0.1311, -0.1263],\n",
              "          ...,\n",
              "          [-0.1132, -0.0887, -0.0922,  ..., -0.0896, -0.0930, -0.0902],\n",
              "          [-0.1176, -0.1493, -0.1308,  ..., -0.1459, -0.1332, -0.1269],\n",
              "          [-0.1156, -0.1119, -0.1025,  ..., -0.1133, -0.1023, -0.1081]],\n",
              "\n",
              "         [[ 0.0940,  0.0955,  0.0792,  ...,  0.0936,  0.0787,  0.0970],\n",
              "          [ 0.0904,  0.0928,  0.0776,  ...,  0.0902,  0.0749,  0.0855],\n",
              "          [ 0.0861,  0.0901,  0.0721,  ...,  0.0889,  0.0744,  0.0948],\n",
              "          ...,\n",
              "          [ 0.0907,  0.0987,  0.0756,  ...,  0.0918,  0.0742,  0.0846],\n",
              "          [ 0.0860,  0.0887,  0.0775,  ...,  0.0891,  0.0761,  0.0942],\n",
              "          [ 0.0848,  0.1054,  0.0685,  ...,  0.1059,  0.0680,  0.0967]]]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeeGssQPQv4v",
        "outputId": "570a166b-d719-4222-bed9-7606aab014dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[ 0.2941,  0.2941,  0.2863,  ...,  0.2471,  0.2549,  0.2627],\n",
              "          [ 0.2863,  0.2863,  0.2784,  ...,  0.2471,  0.2549,  0.2627],\n",
              "          [ 0.2784,  0.2784,  0.2706,  ...,  0.2392,  0.2549,  0.2627],\n",
              "          ...,\n",
              "          [ 0.0039,  0.0039,  0.0118,  ...,  0.1137,  0.1451,  0.2000],\n",
              "          [ 0.0039,  0.0118,  0.0118,  ...,  0.1059,  0.0196, -0.1059],\n",
              "          [ 0.0118,  0.0118,  0.0118,  ...,  0.0275, -0.1373, -0.3647]],\n",
              "\n",
              "         [[ 0.1059,  0.1059,  0.0980,  ...,  0.0667,  0.0745,  0.0824],\n",
              "          [ 0.0980,  0.0980,  0.0902,  ...,  0.0667,  0.0745,  0.0824],\n",
              "          [ 0.0902,  0.0902,  0.0824,  ...,  0.0588,  0.0745,  0.0824],\n",
              "          ...,\n",
              "          [-0.1216, -0.1216, -0.1137,  ..., -0.0039,  0.0275,  0.0824],\n",
              "          [-0.1216, -0.1137, -0.1137,  ..., -0.0118, -0.0980, -0.2235],\n",
              "          [-0.1137, -0.1137, -0.1137,  ..., -0.0902, -0.2549, -0.4824]],\n",
              "\n",
              "         [[-0.0980, -0.0980, -0.1059,  ..., -0.2000, -0.1922, -0.1843],\n",
              "          [-0.1059, -0.1059, -0.1137,  ..., -0.2000, -0.1922, -0.1843],\n",
              "          [-0.1137, -0.1137, -0.1216,  ..., -0.2078, -0.1922, -0.1843],\n",
              "          ...,\n",
              "          [-0.3804, -0.3804, -0.3725,  ..., -0.0510, -0.0039,  0.0431],\n",
              "          [-0.3804, -0.3725, -0.3725,  ..., -0.0431, -0.1216, -0.2471],\n",
              "          [-0.3725, -0.3725, -0.3725,  ..., -0.1216, -0.2784, -0.5059]]]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF3HX1FRkQpO"
      },
      "source": [
        "### output tensor 다시 이미지로 변환하고 결과물 저장하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wboNI6rnh2Oy"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  # Perform translation\n",
        "  output_tensor = cycle_gan.G_XtoY(input_tensor)  # Assuming you want to translate to domain Y\n",
        "\n",
        "  # Convert the output tensor to a PIL image\n",
        "  output_ts = output_tensor.squeeze().cuda()\n",
        "  #output_ts1 = output_ts * 0.5 + 0.5\n",
        "  output_image = transforms.ToPILImage()(output_ts)\n",
        "\n",
        "  # Display or save the translated image\n",
        "  output_image.show()\n",
        "\n",
        "  # Or save it to a file\n",
        "  output_image.save('/content/drive/MyDrive/test/translated_image5.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiBD5FU6W8Vo",
        "outputId": "36878085-1c41-4788-b172-bc5227a67bac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 256, 256])"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_ts1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj3xd5klVXUE",
        "outputId": "7cfda6d3-110e-4177-b591-9a259f4c4528"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 256, 256])"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_tensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "bupmaPhqWLpE",
        "outputId": "660ee4e0-5bff-40e4-8d54-5173797a8d48"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=256x256>"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCcezC7zRMYk",
        "outputId": "22009a9a-2cf3-4fcc-c223-82be2c721da5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor min: -0.15204200148582458\n",
            "Tensor max: 0.10897466540336609\n",
            "Tensor mean: -0.052812766283750534\n",
            "Tensor shape: torch.Size([3, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Perform translation\n",
        "    output_tensor = cycle_gan.G_XtoY(input_tensor)  # Assuming you want to translate to domain Y\n",
        "\n",
        "    # Squeeze the output tensor and move to CPU if necessary\n",
        "    output_ts = output_tensor.squeeze().cuda()\n",
        "\n",
        "    # Check tensor values\n",
        "    print(f'Tensor min: {output_ts.min()}')\n",
        "    print(f'Tensor max: {output_ts.max()}')\n",
        "    print(f'Tensor mean: {output_ts.mean()}')\n",
        "    print(f'Tensor shape: {output_ts.shape}')\n",
        "\n",
        "    # Denormalize the tensor (assuming the model output is in the range [-1, 1])\n",
        "    output_ts = (output_ts + 1) * 0.5\n",
        "\n",
        "    # Optionally scale to [0, 255]\n",
        "    output_ts = output_ts\n",
        "\n",
        "    # Convert the tensor to a PIL image\n",
        "    output_image = transforms.ToPILImage()(output_ts)\n",
        "\n",
        "    # Display or save the translated image\n",
        "    output_image.show()\n",
        "\n",
        "    # Or save it to a file\n",
        "    output_image.save('/content/drive/MyDrive/deep_learning/translated_image4.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6XOPiKoQ0Zd",
        "outputId": "a980ca1f-8785-4411-aa1b-b4fbf441ea8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[0.4350, 0.4315, 0.4333,  ..., 0.4317, 0.4332, 0.4365],\n",
              "         [0.4330, 0.4509, 0.4322,  ..., 0.4512, 0.4324, 0.4442],\n",
              "         [0.4297, 0.4311, 0.4290,  ..., 0.4285, 0.4295, 0.4340],\n",
              "         ...,\n",
              "         [0.4334, 0.4512, 0.4313,  ..., 0.4511, 0.4319, 0.4452],\n",
              "         [0.4298, 0.4298, 0.4303,  ..., 0.4278, 0.4301, 0.4355],\n",
              "         [0.4308, 0.4459, 0.4301,  ..., 0.4447, 0.4297, 0.4393]],\n",
              "\n",
              "        [[0.4475, 0.4407, 0.4479,  ..., 0.4394, 0.4490, 0.4467],\n",
              "         [0.4438, 0.4549, 0.4536,  ..., 0.4543, 0.4552, 0.4555],\n",
              "         [0.4407, 0.4262, 0.4356,  ..., 0.4284, 0.4344, 0.4368],\n",
              "         ...,\n",
              "         [0.4434, 0.4557, 0.4539,  ..., 0.4552, 0.4535, 0.4549],\n",
              "         [0.4412, 0.4253, 0.4346,  ..., 0.4270, 0.4334, 0.4365],\n",
              "         [0.4422, 0.4440, 0.4487,  ..., 0.4433, 0.4488, 0.4459]],\n",
              "\n",
              "        [[0.5470, 0.5478, 0.5396,  ..., 0.5468, 0.5394, 0.5485],\n",
              "         [0.5452, 0.5464, 0.5388,  ..., 0.5451, 0.5374, 0.5427],\n",
              "         [0.5430, 0.5450, 0.5361,  ..., 0.5444, 0.5372, 0.5474],\n",
              "         ...,\n",
              "         [0.5453, 0.5493, 0.5378,  ..., 0.5459, 0.5371, 0.5423],\n",
              "         [0.5430, 0.5444, 0.5387,  ..., 0.5445, 0.5380, 0.5471],\n",
              "         [0.5424, 0.5527, 0.5342,  ..., 0.5530, 0.5340, 0.5483]]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "lh22wTbUPXrw",
        "outputId": "dbe7f985-c88f-48c4-cd69-48b48bc8747e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=256x256>"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FB2Sc3BDQLUa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}